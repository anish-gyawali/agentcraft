# AgentCraft ðŸ¤–âš¡

> Local multi-agent AI copilot â€” runs entirely on your hardware, no paid APIs.

Built to understand codebases, perform research, use tools, and maintain memory
using open-source LLMs orchestrated with LangGraph.

## Architecture
- **Agent Orchestration** â€” LangGraph multi-agent workflows
- **Local LLMs** â€” Ollama (Mistral, CodeLlama) on GPU
- **Vector Memory** â€” ChromaDB + nomic-embed-text embeddings
- **Tool Use** â€” Git, filesystem, code execution
- **Backend API** â€” FastAPI
- **Frontend** â€” React + Vite

## Tech Stack
| Layer | Technology |
|-------|-----------|
| LLM Runtime | Ollama |
| Agent Framework | LangChain + LangGraph |
| Vector DB | ChromaDB |
| Backend | FastAPI (Python 3.11) |
| Frontend | React + Vite |
| Deep Learning | PyTorch (CUDA 12.8) |

## Hardware
- GPU: NVIDIA RTX 5070 (12GB VRAM)
- RAM: 32GB
- CPU: AMD Ryzen 9 7900X

## Project Progress
- [x] Phase 0 â€” Environment Setup (WSL2, CUDA, PyTorch)
- [x] Phase 1 â€” Local LLM Running (Ollama, Mistral, CodeLlama)
- [x] Phase 2 â€” Agent Framework (LangChain, LangGraph, Tool Use)
- [x] Phase 3 â€” Vector Memory & RAG
- [x] Phase 4 â€” Full System (API + React UI)
- [ ] Phase 5 â€” Evaluation & Fine-tuning

## Getting Started
### Prerequisites
- NVIDIA GPU with CUDA support
- Ollama installed
- Python 3.11+
- Node.js 18+

### Backend Setup
```bash
conda create -n aidev python=3.11
conda activate aidev
pip install -r backend/requirements.txt
ollama pull mistral && ollama pull codellama
uvicorn backend.main:app --reload
```

### Frontend Setup
```bash
cd frontend
npm install
npm run dev
```